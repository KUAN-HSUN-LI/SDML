{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可能會有的問題\n",
    "* max_len 不要設太大\n",
    "* 要改bert傳入的東西要先讀讀他的source code\n",
    "* predict, plot還沒寫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* python >= 3.7\n",
    "* pytorch >= 1.0\n",
    "* transformers\n",
    "* pandas\n",
    "* nltk\n",
    "* numpy\n",
    "* sklearn\n",
    "* pickle\n",
    "* tqdm\n",
    "* json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f793c10ba90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a  view of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>Task 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6995</td>\n",
       "      <td>D06996</td>\n",
       "      <td>RUM: network Representation learning throUgh M...</td>\n",
       "      <td>We have witnessed the discovery of many techni...</td>\n",
       "      <td>Yu/Lu/Liu/Zhao/Wen/Zheng</td>\n",
       "      <td>cs.LG/cs.SI</td>\n",
       "      <td>2017-10-08</td>\n",
       "      <td>THEORETICAL ENGINEERING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6996</td>\n",
       "      <td>D06997</td>\n",
       "      <td>Towards Zero-Shot Frame Semantic Parsing for D...</td>\n",
       "      <td>State-of-the-art slot filling models for goal-...</td>\n",
       "      <td>Bapna/Tur/Hakkani-Tur/Heck</td>\n",
       "      <td>cs.AI/cs.CL</td>\n",
       "      <td>2017-07-07</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6997</td>\n",
       "      <td>D06998</td>\n",
       "      <td>Efficient Dense Modules of Asymmetric Convolut...</td>\n",
       "      <td>Real-time semantic segmentation plays an impor...</td>\n",
       "      <td>Lo/Hang/Chan/Lin</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6998</td>\n",
       "      <td>D06999</td>\n",
       "      <td>Network Vector: Distributed Representations of...</td>\n",
       "      <td>We propose a neural embedding algorithm called...</td>\n",
       "      <td>Wu/Lerman</td>\n",
       "      <td>cs.SI/cs.LG</td>\n",
       "      <td>2017-09-07</td>\n",
       "      <td>THEORETICAL EMPIRICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6999</td>\n",
       "      <td>D07000</td>\n",
       "      <td>Asymptotic Analysis of Spatial Coupling Coding...</td>\n",
       "      <td>Compute-and-forward (CAF) relaying is effectiv...</td>\n",
       "      <td>Takabe/Wadayama/Hayashi</td>\n",
       "      <td>cs.IT/math.IT</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id                                              Title  \\\n",
       "6995  D06996  RUM: network Representation learning throUgh M...   \n",
       "6996  D06997  Towards Zero-Shot Frame Semantic Parsing for D...   \n",
       "6997  D06998  Efficient Dense Modules of Asymmetric Convolut...   \n",
       "6998  D06999  Network Vector: Distributed Representations of...   \n",
       "6999  D07000  Asymptotic Analysis of Spatial Coupling Coding...   \n",
       "\n",
       "                                               Abstract  \\\n",
       "6995  We have witnessed the discovery of many techni...   \n",
       "6996  State-of-the-art slot filling models for goal-...   \n",
       "6997  Real-time semantic segmentation plays an impor...   \n",
       "6998  We propose a neural embedding algorithm called...   \n",
       "6999  Compute-and-forward (CAF) relaying is effectiv...   \n",
       "\n",
       "                         Authors     Categories Created Date  \\\n",
       "6995    Yu/Lu/Liu/Zhao/Wen/Zheng    cs.LG/cs.SI   2017-10-08   \n",
       "6996  Bapna/Tur/Hakkani-Tur/Heck    cs.AI/cs.CL   2017-07-07   \n",
       "6997            Lo/Hang/Chan/Lin          cs.CV   2018-09-17   \n",
       "6998                   Wu/Lerman    cs.SI/cs.LG   2017-09-07   \n",
       "6999     Takabe/Wadayama/Hayashi  cs.IT/math.IT   2018-07-03   \n",
       "\n",
       "                       Task 2  \n",
       "6995  THEORETICAL ENGINEERING  \n",
       "6996              ENGINEERING  \n",
       "6997              ENGINEERING  \n",
       "6998    THEORETICAL EMPIRICAL  \n",
       "6999              THEORETICAL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('../data/task2_trainset.csv', dtype=str)\n",
    "dataset.head()\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Id**: 流水號  \n",
    "**Title**: 論文標題  \n",
    "**Abstract**: 論文摘要內容, 句子間以 **$$$** 分隔  \n",
    "**Authors**: 論文作者  \n",
    "**Categories**: 論文類別  \n",
    "**Created date**: 論文上傳日期  \n",
    "**Task 2**: 論文分類類別, 若句子有多個類別,以 **空格** 分隔 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 刪除多於資訊 (Remove redundant information)  \n",
    "我們在資料集中保留了許多額外資訊供大家使用，但是在這次的教學中我們並沒有用到全部資訊，因此先將多餘的部分先抽走。  \n",
    "In dataset, we reserved lots of information. But in this tutorial, we don't need them, so we need to discard them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Task 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>D00001</td>\n",
       "      <td>Rapid popularity of Internet of Things (IoT) a...</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D00002</td>\n",
       "      <td>In this paper, we address the problem of compu...</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>D00003</td>\n",
       "      <td>High quality upsampling of sparse 3D point clo...</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>D00004</td>\n",
       "      <td>Internet is the main source of information now...</td>\n",
       "      <td>EMPIRICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>D00005</td>\n",
       "      <td>Automated Facial Expression Recognition (FER) ...</td>\n",
       "      <td>ENGINEERING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>D00006</td>\n",
       "      <td>This paper proposes the continuous semantic to...</td>\n",
       "      <td>THEORETICAL EMPIRICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>D00007</td>\n",
       "      <td>Existing deep multitask learning (MTL) approac...</td>\n",
       "      <td>THEORETICAL EMPIRICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>D00008</td>\n",
       "      <td>In this paper we explore the use of electrical...</td>\n",
       "      <td>ENGINEERING EMPIRICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>D00009</td>\n",
       "      <td>How spiking networks are able to perform proba...</td>\n",
       "      <td>ENGINEERING EMPIRICAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>D00010</td>\n",
       "      <td>Low-density parity-check (LDPC) codes on symme...</td>\n",
       "      <td>THEORETICAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                           Abstract  \\\n",
       "0  D00001  Rapid popularity of Internet of Things (IoT) a...   \n",
       "1  D00002  In this paper, we address the problem of compu...   \n",
       "2  D00003  High quality upsampling of sparse 3D point clo...   \n",
       "3  D00004  Internet is the main source of information now...   \n",
       "4  D00005  Automated Facial Expression Recognition (FER) ...   \n",
       "5  D00006  This paper proposes the continuous semantic to...   \n",
       "6  D00007  Existing deep multitask learning (MTL) approac...   \n",
       "7  D00008  In this paper we explore the use of electrical...   \n",
       "8  D00009  How spiking networks are able to perform proba...   \n",
       "9  D00010  Low-density parity-check (LDPC) codes on symme...   \n",
       "\n",
       "                  Task 2  \n",
       "0            THEORETICAL  \n",
       "1            THEORETICAL  \n",
       "2            ENGINEERING  \n",
       "3              EMPIRICAL  \n",
       "4            ENGINEERING  \n",
       "5  THEORETICAL EMPIRICAL  \n",
       "6  THEORETICAL EMPIRICAL  \n",
       "7  ENGINEERING EMPIRICAL  \n",
       "8  ENGINEERING EMPIRICAL  \n",
       "9            THEORETICAL  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料切割  (Partition)\n",
    "在訓練時，我們需要有個方法去檢驗訓練結果的好壞，因此需要將訓練資料切成training/validataion set。   \n",
    "While training, we need some method to exam our model's performance, so we divide our training data into training/validataion set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "trainset.to_csv('trainset.csv', index=False)\n",
    "validset.to_csv('validset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/task2_public_testset.csv', dtype=str)\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset.to_csv('testset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料格式化 (Data formatting)  \n",
    "有了字典後，接下來我們要把資料整理成一筆一筆，把input的句子轉成數字，把答案轉成onehot的形式。  \n",
    "這裡，我們一樣使用`multiprocessing`來加入進行。  \n",
    "After building dictionary, that's mapping our sentences into number array, and convert answers to onehot format.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1013 00:59:19.835834 140161503090368 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1013 00:59:19.874181 140161503090368 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "NUM_LABLES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1013 00:59:23.988772 140161503090368 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/student/06/b06902124/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小： 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "vocab = tokenizer.vocab\n",
    "print(\"字典大小：\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def label_to_onehot(labels):\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {'THEORETICAL': 0, 'ENGINEERING':1, 'EMPIRICAL':2, 'OTHERS':3}\n",
    "    onehot = [0,0,0,0]\n",
    "    for l in labels.split():\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "        \n",
    "def sentence_to_indices(sent, tokenizer):\n",
    "    \"\"\" Convert sentence to its word indices.\n",
    "    Args:\n",
    "        sentence (str): One string.\n",
    "    Return:\n",
    "        indices (list of int): List of word indices.\n",
    "    \"\"\"\n",
    "    return [tokenizer.convert_tokens_to_ids(word) for word in tokenizer.tokenize(sent)]\n",
    "    \n",
    "def get_dataset(data_path, tokenizer, n_workers=4):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "    results = [None] * n_workers\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for i in range(n_workers):\n",
    "            batch_start = (len(dataset) // n_workers) * i\n",
    "            if i == n_workers - 1:\n",
    "                batch_end = len(dataset)\n",
    "            else:\n",
    "                batch_end = (len(dataset) // n_workers) * (i + 1)\n",
    "            \n",
    "            batch = dataset[batch_start: batch_end]\n",
    "            results[i] = pool.apply_async(preprocess_samples, args=(batch, tokenizer))\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    processed = []\n",
    "    for result in results:\n",
    "        processed += result.get()\n",
    "    return processed\n",
    "\n",
    "def preprocess_samples(dataset, tokenizer):\n",
    "    \"\"\" Worker function.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict)\n",
    "    Returns:\n",
    "        list of processed dict.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        processed.append(preprocess_sample(sample[1], tokenizer))\n",
    "\n",
    "    return processed\n",
    "\n",
    "def preprocess_sample(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    processed = {}\n",
    "    processed['tokens'] = [sentence_to_indices(sent, tokenizer) \n",
    "                           for sent in data['Abstract'].split('$$$')]\n",
    "    processed['tokens'] = sum(processed['tokens'], [])\n",
    "    processed['tokens'] = [tokenizer.convert_tokens_to_ids('[CLS]')] + processed['tokens'] + [tokenizer.convert_tokens_to_ids('[SEP]')]\n",
    "    processed['segments'] = [0] * len(processed['tokens'])\n",
    "    \n",
    "    if 'Task 2' in data:\n",
    "        processed['Label'] = label_to_onehot(data['Task 2'])\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start processing trainset...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[INFO] Start processing validset...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[INFO] Start processing testset...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Start processing trainset...')\n",
    "train = get_dataset('trainset.csv', tokenizer, n_workers=8)\n",
    "print('[INFO] Start processing validset...')\n",
    "valid = get_dataset('validset.csv', tokenizer, n_workers=8)\n",
    "print('[INFO] Start processing testset...')\n",
    "test = get_dataset('testset.csv', tokenizer, n_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [101, 1996, 2001, 8043, 8602, 12046, 2030, 3011, 2693, 2099, 1005, 1055, 3292, 1006, 7861, 2094, 1007, 2003, 1037, 6179, 6994, 1999, 6747, 1010, 3698, 4083, 1998, 3274, 2671, 2007, 2116, 5097, 2000, 6897, 2030, 2966, 12126, 1010, 2426, 2500, 1012, 2926, 1999, 1996, 2422, 1997, 6233, 3375, 2951, 1010, 1996, 22334, 1997, 2122, 12103, 3081, 15502, 3665, 2003, 2411, 1996, 14879, 5387, 1012, 4427, 2011, 2023, 4119, 1010, 1037, 3528, 1997, 2047, 8107, 2000, 15502, 3665, 2038, 2042, 3818, 1999, 3522, 2086, 1998, 2247, 2007, 2122, 2047, 4725, 3310, 1996, 2342, 2005, 1037, 15902, 7831, 1012, 1999, 2023, 3259, 1010, 2057, 8970, 1037, 6847, 10665, 2005, 16246, 15502, 3665, 1010, 2170, 11089, 10665, 1010, 2029, 2003, 2881, 2000, 3710, 2004, 1037, 8699, 3074, 1997, 3471, 1010, 2073, 16246, 15502, 3665, 4725, 2064, 2022, 7718, 1010, 4102, 2000, 2028, 2178, 1010, 1998, 2716, 2000, 2037, 6537, 2006, 2312, 1011, 4094, 12107, 1012, 2009, 3774, 1997, 1037, 3528, 1997, 3897, 15782, 2571, 4871, 1010, 1999, 2536, 18853, 1998, 4280, 1010, 2107, 2004, 2195, 4127, 1997, 18154, 7013, 4871, 1010, 4556, 3231, 4871, 1998, 2613, 2951, 2013, 29105, 1012, 2247, 2007, 1996, 11089, 10665, 2057, 2556, 1037, 5002, 1998, 1037, 2836, 3231, 2005, 1037, 2892, 2930, 1997, 2511, 4725, 7478, 2013, 2062, 3151, 13792, 1010, 2107, 2004, 1996, 5193, 3722, 2595, 1010, 2000, 3728, 2764, 8107, 1010, 2107, 2004, 1996, 25553, 5101, 4118, 1010, 1998, 2164, 2036, 1037, 7831, 2007, 3293, 9611, 2869, 1012, 102], 'segments': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Label': [0, 0, 1, 0]}\n",
      "(6300, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train[0])\n",
    "print(trainset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# max_len sentence\n",
    "#m = 0\n",
    "cnt_castrated_case = 0\n",
    "for t in test:\n",
    "    if len(t['tokens']) > 512:\n",
    "        #m = len(t['tokens'])\n",
    "        cnt_castrated_case += 1\n",
    "print(cnt_castrated_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料封裝 (Data packing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "為了更方便的進行batch training，我們將會借助[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)。  \n",
    "而要將資料放入dataloader，我們需要繼承[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)，撰寫適合這份dataset的class。  \n",
    "`collate_fn`用於batch data的後處理，在`dataloder`將選出的data放進list後會呼叫collate_fn，而我們會在此把sentence padding到同樣的長度，才能夠放入torch tensor (tensor必須為矩陣)。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily training in batch, we'll use `dataloader`, which is a function built in Pytorch[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)  \n",
    "To use datalaoder, we need to packing our data into class `dataset` [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)  \n",
    "`collate_fn` is used for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data, max_len = 512):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, datas):\n",
    "        # get max length in this batch\n",
    "        max_len = max([min(len(data['tokens']), self.max_len) for data in datas])\n",
    "        batch_tokens = []\n",
    "        batch_segments = []\n",
    "        batch_masks = []\n",
    "        batch_label = []\n",
    "        for data in datas:\n",
    "            # padding abstract to make them in same length\n",
    "            abstract_len = len(data['tokens'])\n",
    "            if abstract_len > max_len:\n",
    "                batch_tokens.append(data['tokens'][:max_len])\n",
    "                batch_segments.append(data['segments'][:max_len])\n",
    "                batch_masks.append([1] * max_len)\n",
    "            else:\n",
    "                batch_tokens.append(data['tokens'] + [0] * (max_len-abstract_len))\n",
    "                batch_segments.append(data['segments'] + [0] * (max_len-abstract_len))\n",
    "                batch_masks.append([1] * abstract_len  + [0] * (max_len-abstract_len))\n",
    "            # gather labels\n",
    "            if 'Label' in data:\n",
    "                batch_label.append(data['Label'])\n",
    "        return torch.LongTensor(batch_tokens), torch.LongTensor(batch_segments), torch.LongTensor(batch_masks), torch.FloatTensor(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = BertDataset(train)\n",
    "validData = BertDataset(valid)\n",
    "testData = BertDataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultiLabelSequenceClassification(BertPreTrainedModel):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size + 24, config.num_labels)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, node_vec, token_type_ids=None, attention_mask=None, \n",
    "                position_ids=None, head_mask=None, labels=None):\n",
    "        \n",
    "        outputs = self.bert(input_ids, \n",
    "                            token_type_ids = token_type_ids, \n",
    "                            attention_mask = attention_mask, \n",
    "                            position_ids=None, head_mask=None)\n",
    "        \n",
    "        outputs = self.dropout(outputs[1])\n",
    "        #print('drop: ', outputs.shape, 'node: ', node_vec.shape)\n",
    "        #print(outputs.shape)\n",
    "        logits = self.classifier(torch.cat((outputs, node_vec), 1))\n",
    "        \n",
    "        #outputs = (logits,) + outputs[1:]\n",
    "        \n",
    "        return logits#outputs[0]\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1013 00:59:55.676883 140161503090368 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/student/06/b06902124/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1013 00:59:55.680915 140161503090368 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1013 00:59:56.620226 140161503090368 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/student/06/b06902124/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I1013 01:00:02.862182 140161503090368 modeling_utils.py:405] Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1013 01:00:02.863348 140161503090368 modeling_utils.py:408] Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "model = BertForMultiLabelSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels = 4)\n",
    "# model.freeze_bert_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMultiLabelSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=792, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定使用的運算裝置  \n",
    "Designate running device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env CUDA_VISIBLE_DEVICES=1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義一個算分公式, 讓我們在training能快速了解model的效能\n",
    "Define score function, let us easily observe model performance while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1():\n",
    "    def __init__(self):\n",
    "        self.threshold = 0.0\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "        self.name = 'F1'\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "\n",
    "    def update(self, predicts, groundTruth):\n",
    "        predicts = predicts > self.threshold\n",
    "        self.n_precision += torch.sum(predicts).data.item()\n",
    "        self.n_recall += torch.sum(groundTruth).data.item()\n",
    "        self.n_corrects += torch.sum(groundTruth.type(torch.bool) * predicts).data.item()\n",
    "\n",
    "    def get_score(self):\n",
    "        recall = self.n_corrects / self.n_recall\n",
    "        precision = self.n_corrects / (self.n_precision + 1e-20)\n",
    "        return 2 * (recall * precision) / (recall + precision + 1e-20)\n",
    "\n",
    "    def print_score(self):\n",
    "        score = self.get_score()\n",
    "        return '{:.5f}'.format(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "EPOCHS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7000, 24])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "################################\n",
    "# for embedding graph features #\n",
    "################################\n",
    "\n",
    "with open('node_vec.pkl', 'rb') as file:\n",
    "    node_tensor = pickle.load(file)\n",
    "#node_tensor = torch.cat((node_tensor, torch.zeros((1, 24)).type(torch.DoubleTensor)), 0)\n",
    "print(node_tensor.shape)\n",
    "\n",
    "train_nodevec, valid_nodevec = node_tensor[:6300], node_tensor[6300:]\n",
    "\n",
    "\n",
    "sample_size = train_nodevec.shape[0]\n",
    "node_trainset = []\n",
    "for i in range(sample_size//BATCH_SIZE):\n",
    "    _tmp = train_nodevec[i*2:i*2+2]\n",
    "    node_trainset.append(_tmp)\n",
    "print('length of training node vector pack: ', len(node_trainset))\n",
    "\n",
    "sample_size = valid_nodevec.shape[0]\n",
    "node_validset = []\n",
    "for i in range(sample_size//BATCH_SIZE):\n",
    "    _tmp = valid_nodevec[i*2:i*2+2]\n",
    "    node_validset.append(_tmp)\n",
    "print('length of validation node vector pack: ', len(node_validset))\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "    \n",
    "def _run_epoch(epoch, training):\n",
    "    model.train(training)\n",
    "    if training:\n",
    "        description = 'Train'\n",
    "        BATCH_SIZE = 2\n",
    "        dataset = trainData\n",
    "        shuffle = True\n",
    "        node_vec = node_trainset\n",
    "    else:\n",
    "        description = 'Valid'\n",
    "        BATCH_SIZE = 2\n",
    "        dataset = validData\n",
    "        shuffle = False\n",
    "        node_vec = node_validset\n",
    "    \n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=shuffle,\n",
    "                            collate_fn=dataset.collate_fn,\n",
    "                            num_workers=4)\n",
    "    \n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n",
    "    loss = 0.0\n",
    "    f1_score = F1()\n",
    "    for i, (tokens, segments, masks, labels) in trange:\n",
    "        o_labels, batch_loss = _run_iter(tokens, segments, masks, labels, node_vec=node_vec[i].type(torch.FloatTensor))\n",
    "        \n",
    "        if training:\n",
    "            opt.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        \n",
    "        loss += batch_loss.item()\n",
    "        f1_score.update(o_labels.cpu(), labels)\n",
    "        \n",
    "        trange.set_postfix(\n",
    "            loss=loss / (i + 1), f1=f1_score.print_score())\n",
    "        \n",
    "    \n",
    "    if training:\n",
    "        history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "    else:\n",
    "        history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "\n",
    "        \n",
    "def _run_iter(tokens, segments, masks, labels, node_vec=torch.zeros(BATCH_SIZE, 0)):\n",
    "    tokens = tokens.to(device)\n",
    "    segments = segments.to(device)\n",
    "    masks = masks.to(device)\n",
    "    labels = labels.to(device)\n",
    "    node_vec = node_vec.to(device)\n",
    "    outputs = model(tokens, node_vec=node_vec, token_type_ids=segments, attention_mask=masks)\n",
    "    l_loss = criteria(outputs, labels)\n",
    "    return outputs, l_loss\n",
    "\n",
    "def save(epoch):\n",
    "    if not os.path.exists('model'):\n",
    "        os.makedirs('model')\n",
    "    torch.save(model.state_dict(), 'model/model-1.pkl.'+str(epoch))\n",
    "    with open('model/history-1.json', 'w') as f:\n",
    "        json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import json\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-5, eps = 1e-8)\n",
    "criteria = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "model.to(device)\n",
    "history = {'train':[],'valid':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35db4ec14ec146488daa03ef8776bbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=3150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350d2747e66f417b82f7f3aa2727f2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Valid', max=350, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8297fccaf7ab4e0584e4ed9432360718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=3150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da906c51cef4f17816eeae448303546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Valid', max=350, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46eb13e2609d4f479bc943def53a8b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=3150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5d52bbbe144a8f90b01ed3614f55e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Valid', max=350, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca48a8ab14ce485bb7cef9f10aa4dfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=3150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4018ec68ec3493f8383c2eac00c1fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Valid', max=350, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c4605132ef4464adf780ebfbd00e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=3150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542ebca1dbc9442ab56d4bce99ff129b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Valid', max=350, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb2d7bc40e24eb98d341ac513627ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=3150, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e01f035b1e4d9fa1fba7bb2c9c55dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Valid', max=350, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    if epoch > 1:\n",
    "        model.freeze_bert_encoder()\n",
    "    _run_epoch(epoch, True)\n",
    "    _run_epoch(epoch, False)\n",
    "    save(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbIAAAE/CAYAAAAjXUYaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXiV9bnv//edeYSQEKYkkDAoozJERFutxWrBAbHWgru/bs8++zi0sh3bSru73Zb2tGp3bfUU27rbnt/pObtFqtXiSNtdh9rjQEAGmZRJCaCEeYYM9/ljPQkrkJAFWcmaPq/rWlfWM63ca4l8+D7Pvb6PuTsiIiKJKi3WBYiIiHSGgkxERBKagkxERBKagkxERBKagkxERBKagkxERBKagkxERBKagkykm5jZJjP7TKzrEEk2CjIREUloCjKRGDOzm8xsnZntMrMFZjYgWG9m9iMz225m+8xshZmNDrZdYWarzGy/mW0xs6/G9l2IxI6CTCSGzGwy8H3gC0B/4ANgXrD5cuBi4CygZ7DPzmDbL4Fb3L0QGA38pRvLFokrGbEuQCTFfRH4lbsvATCzbwC7zawSqAcKgeHA2+6+Ouy4emCkmS1z993A7m6tWiSOaEQmElsDCI3CAHD3A4RGXWXu/hfgJ8BcYLuZPW5mPYJdrwOuAD4ws1fN7IJurlskbijIRGJrKzCoecHM8oESYAuAuz/q7hOAkYROMX4tWL/I3a8B+gDPAPO7uW6RuKEgE+lemWaW0/wAfgv8g5mNNbNs4HvAW+6+yczOM7PzzSwTOAgcAZrMLMvMvmhmPd29HtgHNMXsHYnEmIJMpHu9ABwOe1wC/AvwFLANGALMDPbtAfw7oetfHxA65fiDYNuXgE1mtg+4ldC1NpGUZLqxpoiIJDKNyEREJKEpyEREJKEpyEREJKFFFGRmNsXM1gbT6Mw+xX7XmZmbWXXYunPM7A0zWxlMsZMTjcJFREQggmYPM0sH3gMuA2qBRcAN7r7qhP0KgeeBLGCWu9eYWQawBPiSuy8zsxJgj7s3Rv+tiIhIKopkiqqJwDp33wBgZvOAa4BVJ+z3HeBBgi9sBi4Hlrv7MgB330kHevfu7ZWVlRGUJSIiqWLx4sU73L20rW2RBFkZsDlsuRY4P3wHMxsPVLj782YWHmRnAW5mC4FSYJ67P3TiLzCzm4GbAQYOHEhNTU0EZYmISKowsw/a29bpZg8zSwMeBu5pY3MG8ElCX9b8JHCtmV164k7u/ri7V7t7dWlpm4ErIiLSpkiCbAtQEbZcHqxr1nwbiVfMbBMwCVgQNHzUAq+5+w53P0RoVoPx0ShcREQEIguyRcAwM6sysyxC0+csaN7o7nvdvbe7V7p7JfAmMM3da4CFwBgzywsaPz7FydfWREREzliH18jcvcHMZhEKpXRC905aaWZzgBp3X3CKY3eb2cOEwtCBF9z9+SjVLiKSEurr66mtreXIkSOxLqXL5eTkUF5eTmZmZsTHxN1ci9XV1a5mDxGR4zZu3EhhYSElJSWYWazL6TLuzs6dO9m/fz9VVVWttpnZYnevbus4zewhIhLnjhw5kvQhBmBmlJSUnPbIU0EmIpIAkj3Emp3J+1SQiYjIKe3Zs4fHHnvstI+74oor2LNnTxdU1JqCTERETqm9IGtoaDjlcS+88AJFRUVdVVaLpAyyd7fs5fnl22JdhohIUpg9ezbr169n7NixnHfeeVx00UVMmzaNkSNHAjB9+nQmTJjAqFGjePzxx1uOq6ysZMeOHWzatIkRI0Zw0003MWrUKC6//HIOHz4ctfqSMsh+9Kf3mP3UcnYcOBrrUkREEt4DDzzAkCFDWLp0KT/4wQ9YsmQJjzzyCO+99x4Av/rVr1i8eDE1NTU8+uij7Nx58rS677//PrfddhsrV66kqKiIp556Kmr1RTLXYsL55pUj+OyPXuOHf3yP739uTKzLERGJmm8/u5JVW/dF9TVHDujBv149KuL9J06c2Ko9/tFHH+Xpp58GYPPmzbz//vuUlJS0OqaqqoqxY8cCMGHCBDZt2tT5wgNJOSIbUlrA319QyROLPoz6f3ARkVSXn5/f8vyVV17hz3/+M2+88QbLli1j3LhxbbbPZ2dntzxPT0/v8Pra6UjKERnAHZcO4/fv1DLnuZX89qZJKdO6KiLJ7XRGTtFSWFjI/v3729y2d+9eevXqRV5eHmvWrOHNN9/s5uqSdEQG0DMvk7svO4s3N+xi4cqPY12OiEjCKikp4ROf+ASjR4/ma1/7WqttU6ZMoaGhgREjRjB79mwmTZrU7fUl9RRVDY1NTH3krxxtaOJPd19MdkZ6VF5XRKQ7rV69mhEjRsS6jG7T1vtN2SmqMtLT+JerRvLhrkP8/3/bFOtyRESkCyR1kAFcfFYplw7vw//4yzrq9qsdX0Qk2SR9kEGoHf9IfSMP/2ltrEsREZEoS4kgG1JawI0XVjJv0WZWbt0b63JERCSKUiLIAG6fPIyi3EzmPLuKeGtwERGRM5cyQdYzL5O7Lz+btzbuYuHKj2JdjoiIREnKBBnADedVcFbfAv77C6s52tAY63JERJJSQUEBAFu3buXzn/98m/tccsklROurVikVZM3t+Jt3HeZXr2+KdTkiIkltwIABPPnkk13+e1IqyAAuGlbKZ0b0Ye7L69i+//Rupy0ikopmz57N3LlzW5bvv/9+vvvd73LppZcyfvx4xowZwx/+8IeTjtu0aROjR48G4PDhw8ycOZMRI0Zw7bXX6jYunfXNK0ZwtKGRHy58L9aliIjEvRkzZjB//vyW5fnz53PjjTfy9NNPs2TJEl5++WXuueeeUzbS/fSnPyUvL4/Vq1fz7W9/m8WLF0etvqSdNPhUBpcWcOMFlfzybxv50gWDGF3WM9YliYhE5sXZ8NGK6L5mvzEw9YF2N48bN47t27ezdetW6urq6NWrF/369eOuu+7itddeIy0tjS1btvDxxx/Tr1+/Nl/jtdde4/bbbwfgnHPO4Zxzzola+Sk5IgP4p0uH0Ssvi+88p3Z8EZGOXH/99Tz55JM88cQTzJgxg//4j/+grq6OxYsXs3TpUvr27dvm7Vu6Q0qOyAB65oZmx//WM+/y0rsfMXVM/1iXJCLSsVOMnLrSjBkzuOmmm9ixYwevvvoq8+fPp0+fPmRmZvLyyy/zwQcfnPL4iy++mN/85jdMnjyZd999l+XLl0ettpQdkQHMPK+Cs/sW8r0XV3OkXu34IiLtGTVqFPv376esrIz+/fvzxS9+kZqaGsaMGcOvf/1rhg8ffsrjv/zlL3PgwAFGjBjBfffdx4QJE6JWW1LfxiUSf1u3gy/+4i2+PuVsvnLJ0G77vSIikdJtXFL4Ni6R+MTQ3nxmRF/m/kXt+CIiiSjlgwzgn68cwbHGJv5toWbHFxFJNBEFmZlNMbO1ZrbOzGafYr/rzMzNrPqE9QPN7ICZfbWzBXeFqt75/JcLK/nd4lre3aLZ8UVEEkmHQWZm6cBcYCowErjBzEa2sV8hcAfwVhsv8zDwYudK7VqzJofa8TU7vojEo1T5e+lM3mckI7KJwDp33+Dux4B5wDVt7Pcd4EGg1YUmM5sObARWnnZ13ahnbib3XH4Wb2/axYvvanZ8EYkfOTk57Ny5M+nDzN3ZuXMnOTk5p3VcJN8jKwM2hy3XAueH72Bm44EKd3/ezL4Wtr4AuBe4DIjL04rhZlRX8L/f+IDvvbCaycP7kJOZHuuSREQoLy+ntraWurq6WJfS5XJycigvLz+tYzr9hWgzSyN06vC/tLH5fuBH7n7AzE71GjcDNwMMHDiwsyWdsYz0NO67aiR/94u3+OXrG7nt02rHF5HYy8zMpKqqKtZlxK1ITi1uASrClsuDdc0KgdHAK2a2CZgELAgaPs4HHgrW3wl808xmnfgL3P1xd6929+rS0tIzeiPRcuHQ3lw2si+PvbyO7fvUji8iEu8iCbJFwDAzqzKzLGAmsKB5o7vvdffe7l7p7pXAm8A0d69x94vC1v8Y+J67/yT6byO6/vmKUDv+D9SOLyIS9zoMMndvAGYBC4HVwHx3X2lmc8xsWlcXGAuVvfP5h09U8eSSWlbUqh1fRCSepfwUVe3Zd6SeT//gFQaX5jP/lgs41TU+ERHpWpqi6gz0yMnknsvPZtGm3bywQu34IiLxSkF2CjPOq2B4v0K+94JmxxcRiVcKslNITzPuu3okW/Yc5pevb4x1OSIi0gYFWQcuHNKby0f2Ze7L6/hY7fgiInFHQRaBf75yBPVqxxcRiUsKsggMKsnnv36iiicXqx1fRCTeKMgiNGvyUHoXZPHtZ1cm/cSdIiKJREEWocKgHb/mg908v2JbrMsREZGAguw0fKG6ghH9e/D9F9aoHV9EJE4oyE5Deppx31Whdvxf/HVDrMsREREUZKftgiElTBnVj8deWa92fBGROKAgOwPfuGI4DY3OQy+pHV9EJNYUZGdgUEk+//DJSp5aUsuyzXtiXY6ISEpTkJ2hWZ8OtePPeW6V2vFFRGJIQXaGCnMy+erlZ7P4g908u1zt+CIisaIg64TrqysY2b8HD2h2fBGRmFGQdULz7Phb9x7h319TO76ISCwoyDpp0uASpo4OteN/tFft+CIi3U1BFgXfmDqCxibnoYVrYl2KiEjKUZBFwcCSPP7xoip+v2QLS9WOLyLSrRRkUXLbp4fSuyCbOZodX0SkWynIoqQgO4Ovf/Zslny4hwXLtsa6HBGRlKEgi6LrJpQzakAPHnxxDYePqR1fRKQ7KMiiqHl2/K17j/C42vFFRLqFgizKzh9cwhVj+vGzV9ezbe/hWJcjIpL0FGRd4BtTR9Domh1fRKQ7KMi6QEVxHv/tk1U8/c4W3vlwd6zLERFJagqyLvKVTw+ltDBbs+OLiHSxiILMzKaY2VozW2dms0+x33Vm5mZWHSxfZmaLzWxF8HNytAqPdwXZGXzts2fzjtrxRUS6VIdBZmbpwFxgKjASuMHMRraxXyFwB/BW2OodwNXuPga4Efjf0Sg6UXx+fDmjy3rwgNrxRUS6TCQjsonAOnff4O7HgHnANW3s9x3gQaBl5lx3f8fdm4cjK4FcM8vuZM0JIy3NuO+qUWzbe4Sfv7Y+1uWIiCSlSIKsDNgctlwbrGthZuOBCnd//hSvcx2wxN2PnnaVCWxiVTFXjumvdnwRkS7S6WYPM0sDHgbuOcU+owiN1m5pZ/vNZlZjZjV1dXWdLSnuzJ46nCaHB1/U7PgiItEWSZBtASrClsuDdc0KgdHAK2a2CZgELAhr+CgHngb+3t3bPL/m7o+7e7W7V5eWlp7+u4hzFcV53HRRFc8s3coSteOLiERVRgT7LAKGmVkVoQCbCfxd80Z33wv0bl42s1eAr7p7jZkVAc8Ds939b9Es/JRefQg+Xgl9R0GfEdBnJPSqhLT0bivhRF++ZCjza2qZ8+wqfv/lC0lLs5jVIiISFQ3H4NiB4HEw9Di6P3gerAc47791aRkdBpm7N5jZLGAhkA78yt1XmtkcoMbdF5zi8FnAUOA+M7svWHe5u2/vbOGnVH8Iti2FVc8cX5eRC6Vntw63PiOhsB9Y14dK8+z4X3tyOQuWbWX6uLKODxIRiRb342HTHDJHmwMoCJ82l8OC6sTtjcc6/r05RV0eZBZvX9atrq72mpqa6LzY0QNQtxa2rzr++HgVHAzL0dxex0OtJeBGQG5RdGoI09TkXDP3b9TtP8pfvvop8rIiGRCLSEpqrD9hdNPGaKclWCJZPghE+Pd9ejZkF0BWPmQVhn5GtNz8CF/ODz06ycwWu3t1W9uS+2/S7AIonxB6hDu4A7avbh1uy+aF/pXRrEfZ8VBrHsX1Phsyc864nLQ0476rR3L9z97g569u4K7Lzjrj1xKROOIeOhPU5gjmVMsnBlXY9khGOwBYKDBagiUIkML+UBK2fOL2doOpANIzu/TjirbkDrL25PeGqotCj2busHfz8YD7eFXo+cZXj/+BsjQoHtI63PqMguKqiK+/nVdZzJXn9Ofnr61nxnkVDCjK7YI3KCKdduxQ6OzNwR1wYHvwvA4O1IV+HqwL1tfB4d1EPtrJajtcCvpCdmEbYdM8wgnfFracmdctl0fiWXKfWoyGxnrYtSHUPBI+itu1kZY/uBk5oetvLacog5FcjwFt/gGr3X2IyT98lamj+/HIzHHd+35EUpV7KHAO7giF0oHtx5+3Cqjtoef1B9t+neyeUFAK+WGPvOITgqed8MkqgIys7n3fSSJ1Ty1GQ3pmKKRKz269/tghqFvTOtzWvwzLfnt8n5yerYMtGMWV9+rFzRcN5icvr+PvL6hkwqBe3fueRJJFYz0c2nl8ZBQ+SjppeQc01Z/8GpYGeSWQ3ycUUL3OCz3P7w0FfVo/z+vdqcsL0jUUZGcqKw/Kxoce4Q7tOvn624on4eje4/sUDuDO3sPpl1fAX5+sYdyMaaT1HQ6ZOs0ocvIpvbBR0okBdXhX26+Rnh2EUO/QtaJ+5wSjqD6hEVT487zimH41RzpPpxa7gzvs29o63LavonH7GtKbwq6/9aqCviNbn6IsHgzp+veGJDB3OLInCKJ2TuM1Pz+44/h3j06U3SMIoSCg2gql5m3ZPVL+ulGy0anFWDODnmWhx7DLjq9uqOfLP3mKogPrmDPJyNy5JhR0a54HbwrtlJ4NpWe1Dre+I0NdlfofVWKlsQEO7Tj5VF74daeW53UdnNILrjOVn9f6ulNBn9bLOqUn7VCQxVBaRib/OP1yPv+zQkp9GHfPCNrx6w8H338LO0W56XVY/sTxg7N7Bl2TI46HW5+RodMkIqejqREO7wmdpju8O3R6/PCu4z+b1x3aGcEpvazj15oK+4Wd0is9+bqTTulJlCjIYqy6spirzunPz18NteOXFeWGrpUNGBt6hDu8G7avge3NHZSrYeXTsPh/Ht+noN8JXw8YCaXDQ9f0JLm5h07LHQrCpyWMdrexLmzbkb202zpuaaFJA3KLQ6On3sNg0IXHwyr/hOtOOqUnMaBrZHGgdvchLv3hq3x2VD8eveE02/HdYf9Hx8MtuP5G3RpoaL41nIXmmmwOt6JBoWDLzA+FZmZesJx7fF1Wvv61HEsNx1qHTVsjpPDRU/Pztk7hNcsqhLzmUCoOC6jiNtYFP7N7QFqnb5Ih0mm6RhbnynvlcfPFg/kff1nHjRcOYsKg0zg9aAY9+oceQz9zfH1TI+ze1Kq5hO2rYO2L4BHerTo9KxRymUHIZeWFLbcVfmH7nrQu/Jhgn4yc5P/Xe1NTqNHh8O42Ttu1NUIKTvG11/AAof8u4QHUe9jJoZTb6+SASrDZGkQipRFZnDh4tIHJP3yFfj1yePorn+i62fHrj8CBj0PX4eoPBY/g+bFDrdcdOxhsC34eO2H/lmOCfZoaTrMYa380mJkbQSDmnjpYM/Oi1/HZPAVRuyOkdkZNR/Ycb9xp6/3nFp0igHq1PWrSTA6SgjQiSwD52Rl8/bPDued3y3hm6RY+N768a35RZg70GtQ1r91YHxZ+bQXiobYDs611B7a3Hban63RHlXjYCGl369BqPMXNzTPzj4988oqhZ/kpTtsFz3N66vStSBQoyOLItePK+PUbm3jwpTV8dlQ/8rMT7D9PemYwwoj+nQOA0Gm6hiOnF4gnjiqb1x07GOrAO3FUCa0DqFcllI3r+LRdRnbXvGcR6VCC/U2Z3Jpnx7/up2/w81fXc/flZ3d8UCpJSwuNoLqyA9Ndp+1EEozakeLMhEHFXH3uAH7+2gZqd5/BqTTpHIWYSMJRkMWh2VOHA/DgS2tjXImISPxTkMWhsqJcbrl4MM8u20rNpnZmUBAREUBBFrduvWQIfXtkM+e5VTQ1xddXJERE4omCLE7lZWVw75ThLK/dy+/f2RLrckRE4paCLI5NH1vGuRVFPPTSGg4ePd0vG4uIpAYFWRxLSzPuu2ok2/cf5Wevro91OSIicUlBFucmDOrFNWMH8Lja8UVE2qQgSwD3ThmOGTzw4ppYlyIiEncUZAlgQFEut1w8hOeWb2OR2vFFRFpRkCWIWz41mH49cpjzrNrxRUTCKcgSRF5WBvdOPZsVW/by1JLaWJcjIhI3FGQJ5JpzyxhbUcRDC9eqHV9EJBBRkJnZFDNba2brzGz2Kfa7zszczKrD1n0jOG6tmX02GkWnqubZ8ev2H+WxV9bFuhwRkbjQYZCZWTowF5gKjARuMLORbexXCNwBvBW2biQwExgFTAEeC15PztD4gb2YPnYA//7XjWzepXZ8EZFIRmQTgXXuvsHdjwHzgGva2O87wIPAkbB11wDz3P2ou28E1gWvJ53w9SnDSVM7vogIEFmQlQGbw5Zrg3UtzGw8UOHuz5/usXL6BhTlcuunhvD8im28vVHt+CKS2jrd7GFmacDDwD2deI2bzazGzGrq6uo6W1JKuOXiIfTvmcOc51aqHV9EUlokQbYFqAhbLg/WNSsERgOvmNkmYBKwIGj46OhYANz9cXevdvfq0tLS03sHKSo3K53ZU4fz7pZ9PKl2fBFJYZEE2SJgmJlVmVkWoeaNBc0b3X2vu/d290p3rwTeBKa5e02w30wzyzazKmAY8HbU30WKmnbuAMYNLOIHC9dyQO34IpKiOgwyd28AZgELgdXAfHdfaWZzzGxaB8euBOYDq4CXgNvcvbHzZQuAWWh2/Lr9R3nsZbXji0hqMvf4ur5SXV3tNTU1sS4jodz1xFKeX7GN/7z7U1QU58W6HBGRqDOzxe5e3dY2zeyRBL4+5WzSzfj+i6tjXYqISLdTkCWB/j1D7fgvrPiItzbsjHU5IiLdSkGWJG6+eDADeuYw57lVNKodX0RSiIIsSeRmpXPv1OGs3LqPpxarHV9EUoeCLIlMO3cA4weGZsdXO76IpAoFWRIxM+67ehQ7DhxlrtrxRSRFKMiSzNiKIj43roxfanZ8EUkRCrIk9PUpw0lPM773gtrxRST5KciSUL+eOXz5kiG8+O5HvKl2fBFJcgqyJHXTRUE7/rNqxxeR5KYgS1K5WenMvmIEq7bt48nFmzs+QEQkQSnIktjV5/RnwqBe/GDhWvYfqY91OSIiXUJBlsSaZ8ffceAYc19eH+tyRES6hIIsyZ1bUcTnxpfxq9c38sHOg7EuR0Qk6hRkKeDeoB3/+y+siXUpIiJRpyBLAX175PCVS4bw0sqPeGO92vFFJLkoyFLETRcPpqwoV7Pji0jSUZCliJzMdGZPHc7qbfv4XY3a8UUkeSjIUshV5/SnelAvHnhpDR/vOxLrckREokJBlkLMjAc/fw5H65u4e/5SmnSKUUSSgIIsxQwpLeC+q0fyt3U7+cXrG2JdjohIpynIUtDM8yr47Ki+/GDhWt7dsjfW5YiIdIqCLAWZGQ987hxK8rO5fd47HDqmu0mLSOJSkKWoXvlZPPyFc9m44yDfeU73LRORxKUgS2EXDu3NzRcP5rdvf8hL734U63JERM6IgizF3XPZ2Ywp68ns3y/no71qyReRxKMgS3FZGWn8eOZYjtY3cc/v1JIvIolHQSYMKS3gX4OW/H//q1ryRSSxRBRkZjbFzNaa2Tozm93G9lvNbIWZLTWz181sZLA+08z+V7BttZl9I9pvQKJjxnkVTBnVj3/7o1ryRSSxdBhkZpYOzAWmAiOBG5qDKsxv3H2Mu48FHgIeDtZfD2S7+xhgAnCLmVVGqXaJIjPjgevGhFryf6uWfBFJHJGMyCYC69x9g7sfA+YB14Tv4O77whbzgeYLLQ7km1kGkAscA8L3lThSlJfFwzPOZePOg3znuVWxLkdEJCKRBFkZED5dem2wrhUzu83M1hMakd0erH4SOAhsAz4E/s3dd3WqYulSFw7pzS0XD+G3b29WS76IJISoNXu4+1x3HwLcC3wrWD0RaAQGAFXAPWY2+MRjzexmM6sxs5q6urpolSRn6O7LzlJLvogkjEiCbAtQEbZcHqxrzzxgevD874CX3L3e3bcDfwOqTzzA3R9392p3ry4tLY2scukyWRlpPBK05GuWfBGJd5EE2SJgmJlVmVkWMBNYEL6DmQ0LW7wSeD94/iEwOdgnH5gErOls0dL1BpcWcP+0kfzf9Tt5XC35IhLHOgwyd28AZgELgdXAfHdfaWZzzGxasNssM1tpZkuBu4Ebg/VzgQIzW0koEP+nuy+P+ruQLvGF6gqmju7Hvy1cy4pateSLSHwy9/g6bVRdXe01NTWxLkMCew4dY+ojfyU3M53nbv8keVkZsS5JRFKQmS1295MuTYFm9pAOFOVl8fAXxrJx50HmPKuWfBGJPwoy6dAFQ0q49VNDmLdoMy+u2BbrckREWlGQSUTu+sxZnFPek9m/X8G2vYdjXY6ISAsFmUQk1JI/jvrGJu5+YhmNaskXkTihIJOIVfXO5/6rR/HGhp08/ppa8kUkPijI5LRcX13OFWP68cM/qiVfROKDgkxOi5nxvWvHUFqYzR3zNEu+iMSegkxOm1ryRSSeKMjkjKglX0TihYJMzpha8kUkHijI5IypJV9E4oGCTDpFLfkiEmsKMum08Jb85bV7Yl2OiKQYBZl0mpnx/WvPCVryl3LwqFryRaT7KMgkKnrmZfKjGWPZpJZ8EelmCjKJmkmDS/jyp4bwRI1a8kWk+yjIJKruuuwszg1a8rfuUUu+iHQ9BZlEVWZ6WEv+/KVqyReRLqcgk6ir7J3P/dNG8eaGXfz8tfWxLkdEkpyCTLrE9RPKuXJMfx7+43ss26yWfBHpOgoy6RLNs+T3KczmzifUki8iXUdBJl2mZ14mDwct+d9+dmWsyxGRJKUgky41aXAJX7lkCPNranlBLfki0gUUZNLl7vzMWZxbUcTsp5arJV9Eok5BJl0uMz2NR2aMpbHJuesJteSLSHQpyKRbNLfkv7VxFz97VS35IhI9CjLpNp+fUM6V5/TnR39SS76IRI+CTLqNmfG96aGW/DvmvaOWfBGJCgWZdKvmWfI/3HWI+xeoJV9EOi+iIDOzKWa21szWmdnsNrbfamYrzGypmb1uZiPDtp1jZm+Y2cpgn5xovgFJPOcPLuErlwzld4treX65WvJFpHM6DDIzSwfmAlOBkcAN4UEV+J8NCZEAABBKSURBVI27j3H3scBDwMPBsRnA/wFudfdRwCVAffTKl0R1x2eGMbaiiG/8fjlb1JIvIp0QyYhsIrDO3Te4+zFgHnBN+A7uvi9sMR9o7q++HFju7suC/Xa6e2Pny5ZEF5olXy35ItJ5kQRZGbA5bLk2WNeKmd1mZusJjchuD1afBbiZLTSzJWb29bZ+gZndbGY1ZlZTV1d3eu9AEtagkny+fc1o3lZLvoh0QtSaPdx9rrsPAe4FvhWszgA+CXwx+HmtmV3axrGPu3u1u1eXlpZGqyRJANeNL+OqoCV/qVryReQMRBJkW4CKsOXyYF175gHTg+e1wGvuvsPdDwEvAOPPpFBJTmbGf792DH175HDHvHc4oJZ8ETlNkQTZImCYmVWZWRYwE1gQvoOZDQtbvBJ4P3i+EBhjZnlB48engFWdL1uSSc/cUEv+ZrXki8gZ6DDI3L0BmEUolFYD8919pZnNMbNpwW6zgvb6pcDdwI3BsbsJdTAuApYCS9z9+S54H5LgJlYV85VLhvLk4lqeW7411uWISAIx9/jqFquurvaamppYlyExUN/YxPU/e4P1dQd46c6LKSvKjXVJIhInzGyxu1e3tU0ze0jcaG7Jb2py7pqnlnwRiYyCTOJKS0v+pl389JV1sS5HRBKAgkziTktL/p/f550Pd8e6HBGJcwoyiTvNLfn9euRwx7ylaskXkVNSkElcam7Jr919iH/9g1ryRaR9CjKJWxOrirnt00N5akktzy5TS76ItE1BJnHt9ktDs+R/8+kV1O4+FOtyRCQOKcgkroW35N/9xDK15IvISRRkEvcGleQzJ2jJf+xlteSLSGsKMkkInxtfxtXnDuDH//k+S9SSLyJhFGSSEMyM704fTb8eOdyplnwRCaMgk4TRMzeTH89US76ItKYgk4RyXmUxs9SSLyJhFGSScG6/dBjjBqolX0RCFGSScDLS03hkxjjcUUu+iCjIJDENLMljzjWj1JIvIgoySVzXjitjmlryRVKegkwSlpnx3WuPt+TvP1If65JEJAYUZJLQeuRk8khzS/4CteSLpCIFmSS86spiZk0exu+XbGGBWvJFUo6CTJLC7ZOHMn5gEf+slnyRlKMgk6SQkZ7GIzNDLfl3PbGUhsamWJckIt1EQSZJo6I4j+9MH8WiTbt57JX1sS5HRLqJgkySyrXjyrlm7AAe+c/3WfyBWvJFUoGCTJLOd6aPpn/PHO584h215IukAAWZJJ0eOZn8eMZYtuw+rFnyRVKAgkySUktL/jtb+MPSLbEuR0S6kIJMklZzS/63nn6XzbvUki+SrCIKMjObYmZrzWydmc1uY/utZrbCzJaa2etmNvKE7QPN7ICZfTVahYt0pKUlH7XkiySzDoPMzNKBucBUYCRww4lBBfzG3ce4+1jgIeDhE7Y/DLwYhXpFTktzS37NB7uZ+7Ja8kWSUSQjsonAOnff4O7HgHnANeE7uPu+sMV8oOUGUWY2HdgI6Kq7xERzS/6jf1FLvkgyiiTIyoDNYcu1wbpWzOw2M1tPaER2e7CuALgX+HbnSxU5c2rJF0leUWv2cPe57j6EUHB9K1h9P/Ajdz9wqmPN7GYzqzGzmrq6umiVJNIivCX/PrXkiySVSIJsC1ARtlwerGvPPGB68Px84CEz2wTcCXzTzGadeIC7P+7u1e5eXVpaGlHhIqerurKYf5o8jKfVki+SVCIJskXAMDOrMrMsYCawIHwHMxsWtngl8D6Au1/k7pXuXgn8GPieu/8kKpWLnIF/mjyUCYN6qSVfJIl0GGTu3gDMAhYCq4H57r7SzOaY2bRgt1lmttLMlgJ3Azd2WcUinZCRnsaPZ4wF4E615IskBXP3jvfqRtXV1V5TUxPrMiTJPfPOFu58Yil3fmYYd37mrFiXIyIdMLPF7l7d1jbN7CEpafq4MqaPHcCj//k+iz/YFetyRKQTFGSSsuZMH82AolzumLeUfWrJF0lYCjJJWT1yMnlk5li27T3Cfc+8G+tyROQMKcgkpU0YVMw/TR7KM0u38sw7askXSUQKMkl5sz49lOpBvfjWM2rJF0lECjJJeRnpafxoxlgMmPn4m/zrH97lhRXb2HngaKxLE5EIZMS6AJF4UFGcx8++NIGfvbqe+TW1/K83PgBgWJ8CJg0uYdLgEiZWFVNamB3jSkXkRPoemcgJjjU0sWLLXt7csJO3Nu6iZtMuDh1rBGBonwLOrypm0uASzh9cTJ/CnBhXK5IaTvU9MgWZSAfqG5t4d8te3tywi7c27mTRxl0cDIJtcGl+KNSCcOvbQ8Em0hUUZCJR1NDYxMqt+1pGbIs27mL/0QYAqnrnM2lwMedXhU5H9uupYBOJBgWZSBdqaGxi1bZ9vLVhF29u2Mnbm3ax/0go2CpL8kKhNiQUbgOKcmNcrUhiUpCJdKPGJmf1ttCI7c0Nu3h74072BcE2sDiv1TW28l55Ma5WJDEoyERiqLHJWfPRvtA1tuB05N7DoSmxynvltrrGVlGsYBNpi4JMJI40NTlrP94fjNh28vbGXew+FAq2sqJczh9czKTgGltFcS5mFuOKRWJPQSYSx5qanPe272+5xvbWxl3sOngMgAE9czh/cElLA8mgkjwFm6QkBZlIAnF33t9+gLeCa2xvbtjJziDY+vXICY3Ygi9pVyrYJEUoyEQSmLuzvu4AbwTX2N7csIsdwfRZfQqzWxpHJg0uYXDvfAWbJCUFmUgScXc27DjY0hX51oadbN8fCrbSwmzOryrm/MElXDC4mCGlBQo2SQqnCjLNtSiSYMyMIaUFDCkt4IvnD8Ld2bjjIG9tDK6xbdjFc8u3AdC7ICv4cnYo3Ib1UbBJ8lGQiSQ4M2NwaQGDSwu4YeJA3J0Pdh7irY3Hr7E9vyIUbCX5WUysOn6NbVifAtLSFGyS2BRkIknGzKjsnU9l73xmnBcKts27DodORW4MjdhefPcjAHrlZXJ+1fFrbGf3LVSwScJRkIkkOTNjYEkeA0vy+MJ5FQBs3nWopdX/zQ07eWllKNiK8jKZWHl85pER/Xoo2CTuKchEUlBFcR4VxXlcXx0Kttrdh1p9j+2Pqz4GoGduJudVFjNpcDGjy3pSnJ9Fz9xMeuZmkpOZHsu3INJCQSYilPfKo3xCHtdNKAdg657DoWts60O3rvnz6o9POiYnM42i3CDY8jIpys2kKC8UckV5WcHPzJZ9ivJC+xVmZ6jhRKJK7fci0qFtew+zbvsB9h6uZ8+hevYerg+eHzthuZ49h49xpL6p3ddKTzN65GS0CrueuaEg7JmXFfrZHIJ5mfTMzWrZJzM9rRvftcQTtd+LSKf075lL/56R34LmSH0j+w7Xsycs+PYcOtYqCPcE63YdPMbGHQfZc6iefUfqOdW/rfOz0lsC8MSwOz4CDI38mkeGRbmZ5GWlaxSYxBRkIhJ1OZnp5GSm0+c075jd2OTsPxI+uguF3b5Wy80jwGOs236APYfr2XuonmON7Y8CM9MtLPyyWoddMOIrysukR27zKdLQPj1yM0lXs0vcU5CJSNxIT7NQiORlMagk8uPcnSP1Tew5fCxsBBgKu/AR4N7g1OfH+4+w9uP97D1U33J37/YU5mS0utZ30vXA3CzyszNITzMy0oz0dCPdgudhj4y0tBOW237evG9aGqGfhkaTHYgoyMxsCvAIkA78wt0fOGH7rcBtQCNwALjZ3VeZ2WXAA0AWcAz4mrv/JYr1i4hgZuRmpZObdXqnQCF0h+99RxpC1/uCsGu5/hcE4r6wU6Fb9x5uGSE2NHVPj0FGmpHWXvhZKDxbgtKC7elGWligtl5OIz0IyvbCtXn5+O9NO+n3p1nodU9eTmupIyczjUvO7tO1n09HO5hZOjAXuAyoBRaZ2QJ3XxW222/c/WfB/tOAh4EpwA7ganffamajgYVAWZTfg4jIGctIT6M4P4vi/KzTOs7dOXiskT2HjnHwaCONTU6TOw1NTmNTEw2NTqM7jU3BuhOWm5rC9m21HHZMG/s2NtFyTGMb+zeG7+u01HK0vun4728Mr9VpaGqiqQkamppO+fvPRHF+Fkv+5bIzOjZSkYzIJgLr3H0DgJnNA64BWoLM3feF7Z8PeLD+nbD1K4FcM8t296OdLVxEJJbMjILsDAqyU+MKjbvT5IQFXVMHQRp6dIdI/guUAZvDlmuB80/cycxuA+4mdBpxchuvcx2wRCEmIpJ4zIx0I6z5JX6+EB+1L2W4+1x3HwLcC3wrfJuZjQIeBG5p61gzu9nMasyspq6uLloliYhICogkyLYAFWHL5cG69swDpjcvmFk58DTw9+6+vq0D3P1xd6929+rS0tIIShIREQmJJMgWAcPMrMrMsoCZwILwHcxsWNjilcD7wfoi4Hlgtrv/LToli4iIHNdhkLl7AzCLUMfhamC+u680szlBhyLALDNbaWZLCV0nu7F5PTAUuM/MlgaPru3DFBGRlKK5FkVEJO6daq5FzcApIiIJTUEmIiIJTUEmIiIJTUEmIiIJTUEmIiIJLe66Fs2sDvggCi/Vm9CkxXIyfTbt02fTPn027dNn075ofTaD3L3NGTPiLsiixcxq2mvVTHX6bNqnz6Z9+mzap8+mfd3x2ejUooiIJDQFmYiIJLRkDrLHY11AHNNn0z59Nu3TZ9M+fTbt6/LPJmmvkYmISGpI5hGZiIikgKQMMjObYmZrzWydmc2OdT3xwsx+ZWbbzezdWNcSb8yswsxeNrNVwZ0c7oh1TfHCzHLM7G0zWxZ8Nt+OdU3xxMzSzewdM3su1rXEGzPbZGYrgjufdNls8El3atHM0oH3gMuAWkL3U7vB3VfFtLA4YGYXAweAX7v76FjXE0/MrD/Q392XmFkhsBiYrj83YGYG5Lv7ATPLBF4H7nD3N2NcWlwws7uBaqCHu18V63riiZltAqrdvUu/Y5eMI7KJwDp33+DuxwjdsfqaGNcUF9z9NWBXrOuIR+6+zd2XBM/3E7r3Xllsq4oPHnIgWMwMHsn1L+AzZGblhG4m/ItY15LKkjHIyoDNYcu16C8kOQ1mVgmMA96KbSXxIzh9thTYDvzJ3fXZhPwY+DrQFOtC4pQDfzSzxWZ2c1f9kmQMMpEzZmYFwFPAne6+L9b1xAt3b3T3sUA5MNHMUv7UtJldBWx398WxriWOfdLdxwNTgduCyxtRl4xBtgWoCFsuD9aJnFJw/ecp4D/c/fexriceufse4GVgSqxriQOfAKYF14HmAZPN7P/EtqT44u5bgp/bgacJXfqJumQMskXAMDOrMrMsYCawIMY1SZwLGhp+Cax294djXU88MbNSMysKnucSaqRaE9uqYs/dv+Hu5e5eSejvmb+4+/8X47LihpnlB41TmFk+cDnQJR3TSRdk7t4AzAIWErpgP9/dV8a2qvhgZr8F3gDONrNaM/vHWNcURz4BfInQv6qXBo8rYl1UnOgPvGxmywn9Q/FP7q5Wc+lIX+B1M1sGvA087+4vdcUvSrr2exERSS1JNyITEZHUoiATEZGEpiATEZGEpiATEZGEpiATEZGEpiATEZGEpiATEZGEpiATEZGE9v8AT4T/GZ5V8RwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score  [0.6944927536231884, 3]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with open('model/history-1.json', 'r') as f:\n",
    "    history = json.loads(f.read())\n",
    "\n",
    "train_loss = [l['loss'] for l in history['train']]\n",
    "valid_loss = [l['loss'] for l in history['valid']]\n",
    "train_f1 = [l['f1'] for l in history['train']]\n",
    "valid_f1 = [l['f1'] for l in history['valid']]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('Loss')\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(valid_loss, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('F1 Score')\n",
    "plt.plot(train_f1, label='train')\n",
    "plt.plot(valid_f1, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Best F1 score ', max([[l['f1'], idx] for idx, l in enumerate(history['valid'])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model/model-1.pkl.{}'.format(0)))\n",
    "model.train(False)\n",
    "# _run_epoch(1, False)\n",
    "dataloader = DataLoader(dataset=testData,\n",
    "                            batch_size=16,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=testData.collate_fn,\n",
    "                            num_workers=4)\n",
    "trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n",
    "prediction = []\n",
    "for i, (tokens, segments, masks, labels) in trange:\n",
    "    with torch.no_grad():\n",
    "        o_labels = model(tokens.to(device), segments.to(device), masks.to(device))\n",
    "        o_labels = o_labels>0.0\n",
    "        prediction.append(o_labels.to('cpu'))\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "#prediction = torch.cat(prediction).detach().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(prediction)\n",
    "for i in a: \n",
    "    if i[3] == 1:\n",
    "        if not (i[0]==0 and i[1]==0 and i[2]==0):\n",
    "            #print(i)\n",
    "            pass\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction-1.csv'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction (numpy array)\n",
    "        sampleFile (str)\n",
    "        public (boolean)\n",
    "        filename (str)\n",
    "    \"\"\"\n",
    "    sample = pd.read_csv(sampleFile)\n",
    "    submit = {}\n",
    "    submit['order_id'] = list(sample.order_id.values)\n",
    "    redundant = len(sample) - prediction.shape[0]\n",
    "    if public:\n",
    "        submit['THEORETICAL'] = list(prediction[:,0]) + [0]*redundant\n",
    "        submit['ENGINEERING'] = list(prediction[:,1]) + [0]*redundant\n",
    "        submit['EMPIRICAL'] = list(prediction[:,2]) + [0]*redundant\n",
    "        submit['OTHERS'] = list(prediction[:,3]) + [0]*redundant\n",
    "    else:\n",
    "        submit['THEORETICAL'] = [0]*redundant + list(prediction[:,0])\n",
    "        submit['ENGINEERING'] = [0]*redundant + list(prediction[:,1])\n",
    "        submit['EMPIRICAL'] = [0]*redundant + list(prediction[:,2])\n",
    "        submit['OTHERS'] = [0]*redundant + list(prediction[:,3])\n",
    "    df = pd.DataFrame.from_dict(submit) \n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubmitGenerator(prediction, sampleFile, public=True, filename='prediction-1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
